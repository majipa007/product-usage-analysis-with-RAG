{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86008ccb-dfdd-4a54-ad30-5975829da794",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "152cc92d-3033-482e-907b-1bfe22e4ae91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.carwale.com/tata-cars/nexon/user-reviews-p2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab87cee9-bd07-4a66-b546-5040c12b702a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.document_loaders.web_base.WebBaseLoader object at 0x7b72506b2f10>\n"
     ]
    }
   ],
   "source": [
    "print(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f1381c0-5c2d-435a-8825-0e142b240b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e71722c-1b03-4bf8-9c2d-2940a9a9e1df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.carwale.com/tata-cars/nexon/user-reviews-p2/', 'title': 'Tata Nexon Reviews - CarWale', 'description': 'Tata Nexon Reviews - Read first-hand reviews from actual Tata Nexon owners. Find out what buyers of Tata Nexon have to say about the car.', 'language': 'en'}, page_content=\"\\n\\nTata Nexon Reviews - CarWale\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\nNEW CARSUSED CARSREVIEWS & NEWSADTata Nexon User ReviewsLooking for Tata Nexon? Here are the reviews and ratings by Nexon owners from across the country.4.6/5391 Ratings5 star75%4 star17%3 star3%2 star1%1 star3%VariantAll VersionsRs. 7,99,990Avg. Ex-ShowroomSelect Your VariantAll VersionsAll VersionsPure 1.2 Petrol 6MTPetrolManualRatings 4.6Smart Plus 1.2 Petrol 5MTPetrolManualRatings 4.6Creative 1.2 Petrol 6MTPetrolManualRatings 4.1Creative 1.2 Petrol 6AMTPetrolAutomaticRatings 4.4Smart Plus (S) 1.2 Petrol 5MTPetrolManualRatings 4.6Pure 1.5 Diesel 6MTDieselManualRatings 5.0Fearless Purple 1.2 Petrol 6MT Dual TonePetrolManualRatings 4.7Fearless Plus (S) Purple 1.5 Diesel 6AMT Dual ToneDieselAutomaticRatings 4.4Pure (S) 1.2 Petrol 6MTPetrolManualRatings 5.0Fearless Plus 1.2 Petrol 7DCA DTPetrolAutomaticRatings 5.0Creative Plus 1.2 Petrol 6MTPetrolManualRatings 4.2Fearless Plus (S) Purple 1.2 Petrol 7DCA Dual TonePetrolAutomaticRatings 4.7Creative 1.2 Petrol 7DCAPetrolAutomaticRatings 4.5Fearless Plus (S) Purple 1.5 Diesel 6MT Dual ToneDieselManualRatings 4.4Fearless (S) Purple 1.2 Petrol 7DCA Dual TonePetrolAutomaticRatings 5.0Creative Plus (S) 1.2 Petrol 6MTPetrolManualRatings 4.8Smart (O) 1.2 Petrol 5MTPetrolManualRatings 4.3Pure (S) 1.5 Diesel 6MTDieselManualRatings 4.0Fearless Plus 1.5 Diesel 6AMT Dual ToneDieselAutomaticRatings 4.8Fearless Plus (S) 1.2 Petrol 6MT Dual TonePetrolManualRatings 4.0Creative Plus (S) 1.2 Petrol 6AMTPetrolAutomaticRatings 4.5Creative Plus (S) 1.2 Petrol 7DCAPetrolAutomaticRatings 4.8Fearless 1.5 Diesel 6MT Dual ToneDieselManualRatings 3.8Smart Plus 1.2 Petrol 6AMTPetrolAutomaticRatings 4.8Fearless Plus (S) 1.2 Petrol 7DCA Dual TonePetrolAutomaticRatings 5.0Creative 1.2 Petrol 6MT Dual TonePetrolManualRatings 5.0Fearless Plus Purple 1.5 Diesel 6MT Dual ToneDieselManualRatings 5.0Smart Plus 1.2 Diesel 6MTDieselManualRatings 4.3Creative Plus (S) 1.2 Petrol 7DCA Dual TonePetrolAutomaticRatings 4.0Creative Plus (S) 1.5 Diesel 6MTDieselManualRatings 5.0Fearless 1.2 Petrol 6MT Dual TonePetrolManualRatings 4.5Fearless Plus (S) 1.2 Petrol 7DCA Dark EditionPetrolAutomaticRatings 5.0Creative 1.5 Diesel 6MT Dual ToneDieselManualRatings 5.0Creative Plus (S) 1.5 Diesel 6MT Dual ToneDieselManualRatings 5.0Creative Plus (S) 1.2 Petrol 6AMT Dual TonePetrolAutomaticRatings 5.0Creative 1.5 Diesel 6MTDieselManualRatings 4.5Creative Plus 1.2 Petrol 7DCAPetrolAutomaticRatings 5.0Creative Plus (S) 1.2 Petrol 6MT Dual TonePetrolManualRatings 4.5Fearless Plus (S) 1.5 Diesel 6AMT Dark EditionDieselAutomaticRatings 5.0Creative Plus 1.5 Diesel 6AMTDieselAutomaticRatings 5.0Creative 1.5 Diesel 6AMT Dual ToneDieselAutomaticRatings 5.0Fearless Plus 1.5 Diesel 6MT Dual ToneDieselManualRatings 5.0Creative 1.2 Petrol 7DCA Dual TonePetrolAutomaticRatings 5.0Fearless (S) Purple 1.5 Diesel 6AMT Dual ToneDieselAutomaticRatings 5.0Creative 1.5 Diesel 6AMT Dark EditionDieselAutomaticRatings 5.0Fearless (S) Purple 1.2 Petrol 6MT Dual TonePetrolManualRatings 1.0Creative 1.2 Petrol 6AMT Dark EditionPetrolAutomaticRatings 1.0Fearless (S) 1.2 Petrol 6MT Dark EditionPetrolManualRatings 4.0Smart Plus (S) 1.2 Diesel 6MTDieselManualRatings 5.0Fearless 1.2 Petrol 6MT Dark EditionPetrolManualRatings 5.0Pure (S) 1.5 Diesel 6AMTDieselAutomaticRatings 5.0Creative Plus 1.5 Diesel 6MT Dual ToneDieselManualRatings 5.0Fearless Plus (S) 1.5 Diesel 6MT Dark EditionDieselManualRatings 5.0Pure 1.2 Petrol 6AMTPetrolAutomaticRatings 4.0Fearless (S) 1.2 Petrol 6MT Dual TonePetrolManualRatings 4.0Creative 1.5 Diesel 6AMTDieselAutomaticRatings 5.0Creative 1.2 Petrol 6AMT Dual TonePetrolAutomaticRatings 1.0Fearless (S) 1.5 Diesel 6MT Dual ToneDieselManualRatings 5.0Creative Plus 1.2 Petrol 6MT Dual TonePetrolManualRatings 5.0Creative Plus 1.2 Petrol 6AMTPetrolAutomaticRatings 5.0Fearless Plus (S) Purple 1.2 Petrol 7DCA DTPetrolAutomaticRatings 5.0Fearless Plus (S) 1.5 Diesel 6AMT Dual ToneDieselAutomaticRatings 5.0Fearless Plus Purple 1.5 Diesel 6AMT Dual ToneDieselAutomaticRatings 5.0Smart 1.2 Revotron Petrol 5MTPetrolManualRatings 4.7XZ Plus (S) Dual TonePetrolManualRatings 4.8Categories\\xa0(out of 5)4.7Exterior4.6Comfort4.5Performance4.3Fuel Economy4.5Value for MoneyAll Tata Nexon Reviews\\xa0(70)All54321Sort by :Most HelpfulMost RecentAll54321Sort by :Most HelpfulMost RecentNexon, its fix on8 months ago | SatishExcellent comfort, easy drive and mainly service also superb. Maintenance also very much reasonable cost. Safety featured vehicle and well on rural areas roads condition. I love my nexon.Rating parameters(out of 5)5Exterior/Styles5Comfort & Space5Performance (Engine/Gear/Overall)5Fuel Economy5Value for Money/FeaturesAbout the ReviewerPurchase  NewDriven forFew hundred kilometersRead MoreWas this review helpful?123Excellent design and safe car for value for money9 months ago | Nagesh NPros Excellent design, good mileage, more safety with latest technology good road grip and good drive mode like city drive model sports mode etc. to get more mileage. Cons Some Tata dealers cheat people and some services center are not upto mark this need to take care.Rating parameters(out of 5)5Exterior/Styles5Comfort & Space5Performance (Engine/Gear/Overall)5Fuel Economy5Value for Money/FeaturesAbout the ReviewerPurchase  Not PurchasedDriven forDid a short drive onceRead MoreWas this review helpful?168Good mileage4 months ago | Rajeshkannan RCar looks good and performance is good mileage could have improved. For family it suits second row has enough space, panel gab is there could have improved manageable . Robustness and reliability, while overtaking it will give more confidentRating parameters(out of 5)5Exterior/Styles5Comfort & Space5Performance (Engine/Gear/Overall)4Fuel Economy4Value for Money/FeaturesAbout the ReviewerPurchase  NewDriven forFew thousand kilometersRead MoreWas this review helpful?146Tata Nexon7 months ago | Rishabh Srivastava Good for the average buyer who is looking for a good family car that can carry 5 people with them self without any extra discomfort good to have any buying experience but needs some fit and finish to be improved.Rating parameters(out of 5)4Exterior/Styles4Comfort & Space3Performance (Engine/Gear/Overall)3Fuel Economy4Value for Money/FeaturesAbout the ReviewerPurchase  NewDriven forFew hundred kilometersRead MoreWas this review helpful?135Tata Nexon review10 months ago | yadvinder chauhanDriving experience excellent. Amazing interior design and exterior too. Sporty and futuristic like a gadgets. Amazing sounds and music. Value for money.Rating parameters(out of 5)5Exterior/Styles5Comfort & Space4Performance (Engine/Gear/Overall)4Fuel Economy5Value for Money/FeaturesAbout the ReviewerPurchase  Not PurchasedDriven forDid a short drive onceRead MoreWas this review helpful?169Top class with safety..8 months ago | BabjiThe riding experience is very good.. we feel confident at any speed and in any road..\\nA.C works very well. \\nCompleted two years and 28 k riding. Still i enjoy the ride.. went long drives with family many times for around 2000 km to 3000 km with comfort riding..\\n Music system is top class.\\nTata service cost also minimal..\\nThank you tata....Rating parameters(out of 5)4Exterior/Styles5Comfort & Space5Performance (Engine/Gear/Overall)4Fuel Economy4Value for Money/FeaturesAbout the ReviewerPurchase  NewDriven forFew thousand kilometersRead MoreWas this review helpful?147New Nexon ownership review10 months ago | Anish All good. Very good car ,very punchy diesel engine, unique looking car, all features and I like it's purple interior looks like customized interior, just loved it ,best of luck for all you guys.Rating parameters(out of 5)5Exterior/Styles5Comfort & Space4Performance (Engine/Gear/Overall)4Fuel Economy5Value for Money/FeaturesAbout the ReviewerPurchase  NewDriven forFew hundred kilometersRead MoreWas this review helpful?159In the year 2030 also this car will feel upto date as per design....top notch....8 months ago | Souvik Das Gupta Buying experience was good....nothing to speak much about styling and looks....its stunning....I have taken the daytona grey color....the smart plus model is the best value for money for those whose on road budget is below 10.50 lakhs....only 7 days have passed by since I have purchased this, car so presently mileage I am getting is approx. 10-12 km/l....will increase eventually for sure.....pros - led headlights and tail lamps....the futuristic design....the practicality....the interior space....thigh support....three point seat belts....6 airbags and 5star safety also we all. Know....boot space has increased.....illuminated logo in the  steering....powerful ac....electrical orvm 's ....fantastic quality of seats.....android and apple Car play.....\\n\\ncons - horn positioning is not good....infotainment screen is like the old one in smart and pure version.....no wheel covers....no back camera....camera I have fixed from showroom separately @ 6000 rupees....overall very small cons and those can be ignored....ultimately a top notch car in my perspective....its definitely a head Turner...Rating parameters(out of 5)5Exterior/Styles5Comfort & Space4Performance (Engine/Gear/Overall)4Fuel Economy5Value for Money/FeaturesAbout the ReviewerPurchase  NewDriven forFew hundred kilometersRead MoreWas this review helpful?104Honest review4 months ago | Sachin Mirji1.Buying experience of car was amazing representative was patient enough to answer all the questions and explain all models and there difference.\\n2.I have rode the car for almost 2500km with full AC on and in different terrain like highways, villages, and Ghar section in that case will not face any problem.\\n3. The car has amazing exterior and interior looks with clean bold presence on road .\\nI got mileage around 20 km/l\\nBut yeah middle seat in back row comfort can be increased no headrest for them and seat is bit elevated.\\n4.had one service did not face any issues regarding that\\n5.pros\\nValue for money\\nDescent performance\\nNice interior\\nLoaded with electronics\\n\\nCons\\nFinishing can be improved\\nMy door biding was not fitted properly ( so finishing has to be worked on)\\nNo headrest for middle passenger\\n\\nThat's all from my side \\nThank youRating parameters(out of 5)5Exterior/Styles4Comfort & Space5Performance (Engine/Gear/Overall)5Fuel Economy5Value for Money/FeaturesAbout the ReviewerPurchase  NewDriven forFew thousand kilometersRead MoreWas this review helpful?93Best car in this Segment2 months ago | VishalThe buying experience is good.  The road presence is good. Highway Experience is also so good. Power-packed Car. Suspension is good & Riding Experience is very good. The Interior looks and feels premium. Nice Ground clearance. Some basic features like the Rear Defogger & wiper are missed and available in only top-end variants. \\nOverall good.Rating parameters(out of 5)5Exterior/Styles5Comfort & Space5Performance (Engine/Gear/Overall)4Fuel Economy4Value for Money/FeaturesAbout the ReviewerPurchase  NewDriven forFew thousand kilometersRead MoreWas this review helpful?82Back1234...7NextReviews you may also considerMahindra XUV 3XO4.68255 rating|Total Reviews: 156Kia Sonet4.4351 rating|Total Reviews: 16Maruti Suzuki Brezza4.48641 rating|Total Reviews: 218Hyundai Venue4.61327 rating|Total Reviews: 97Tata Punch4.321148 rating|Total Reviews: 421Maruti Suzuki Fronx4.54527 rating|Total Reviews: 148Tata Altroz4.591584 rating|Total Reviews: 722Toyota Urban Cruiser Taisor4.63103 rating|Total Reviews: 39Hyundai Exter4.65544 rating|Total Reviews: 134ADTake a closer lookTata NexonRs. 8.00 LakhAvg. Ex-ShowroomWrite a detailed review of a car and you canWin Amazon Voucher worth \\xa0₹2000Write & WinWant to read the reviews for any other car?Select CarHomeTata CarsNexonUser ReviewsLanguage:EnglishहिंदीAbout UsCareersTerms & ConditionsAdvertiseConnect with usDownload Mobile AppCarWale Android AppCarWale Android AppCarWaleBikeWaleCarTradeMobility OutlookOLXabSure©CarTrade Tech.Visitor Agreement\\xa0&\\xa0Privacy Policy\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\")]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc3098-5e4f-4a91-9cf6-de97d31ca650",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0dcd3d3-7868-4ec7-8dee-cead06a851e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sukuna/miniconda3/envs/LLM/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import numpy as np\n",
    "from langchain.vectorstores import Chroma\n",
    "import os\n",
    "import shutil\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1edbb8c4-bb47-4441-b13e-4d5d9beea06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2', device = \"cpu\")\n",
    "class SentenceTransformerEmbeddings:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return self.model.encode(texts).tolist()\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self.model.encode(text).tolist()\n",
    "\n",
    "# Initialize our custom embeddings\n",
    "embeddings = SentenceTransformerEmbeddings(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64124603-4961-4628-b16e-40165ddef990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and extract reviews\n",
    "def extract_reviews(text):\n",
    "    # Extract review blocks\n",
    "    review_pattern = re.compile(r'(\\d+ months ago \\| .+?About the Reviewer)', re.DOTALL)\n",
    "    reviews = review_pattern.findall(text)\n",
    "    \n",
    "    # Clean each review\n",
    "    cleaned_reviews = []\n",
    "    for review in reviews:\n",
    "        # Remove HTML tags and extra whitespace\n",
    "        clean_review = re.sub(r'<.*?>', '', review)\n",
    "        clean_review = re.sub(r'\\s+', ' ', clean_review).strip()\n",
    "        cleaned_reviews.append(clean_review)\n",
    "    \n",
    "    return cleaned_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c94a6fa9-2dda-4093-874e-b469ca231343",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ceee9d1-b666-4534-817f-bb6ce6c589f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_data = model.encode(str(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe995598-28a1-42ae-b7d4-ab9fc0fbf96f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.97421212e-02,  4.56442200e-02,  2.07532048e-02,  4.62603718e-02,\n",
       "       -3.17288050e-03,  2.84598377e-02,  1.63641591e-02,  4.51008528e-02,\n",
       "       -2.05483045e-02, -8.04107711e-02,  7.79745728e-02, -7.45393485e-02,\n",
       "       -1.92617550e-02,  1.67596363e-03, -2.07780022e-02,  4.10910845e-02,\n",
       "        3.42805572e-02, -4.58252132e-02,  9.78834648e-03, -7.18596578e-02,\n",
       "        2.60666925e-02,  3.82510088e-02,  6.50273561e-02, -4.50759940e-03,\n",
       "        5.80998790e-03, -3.17513011e-02, -2.51366328e-02,  1.03435569e-01,\n",
       "       -2.71665002e-03, -4.36415002e-02, -4.18471955e-02,  5.89236394e-02,\n",
       "        5.53052640e-03,  6.42236928e-03, -5.02314372e-03, -6.32848665e-02,\n",
       "       -7.76512325e-02,  1.48405088e-02,  4.05669548e-02, -1.58582162e-02,\n",
       "        2.57344693e-02, -8.80463198e-02, -4.70317602e-02, -4.24393336e-04,\n",
       "        7.13522881e-02, -1.32183656e-02, -7.16199726e-02,  1.12586524e-02,\n",
       "        8.52428749e-02, -4.23713699e-02, -1.22171581e-01, -1.66677777e-02,\n",
       "       -1.56489573e-02, -9.13794711e-02,  4.84165782e-03, -5.98422773e-02,\n",
       "       -3.03077605e-02, -1.74292494e-02,  2.96686385e-02, -2.08962150e-02,\n",
       "        1.02062844e-01,  9.09200311e-03, -3.38318236e-02,  2.15610173e-02,\n",
       "        6.45158440e-02,  1.43766552e-02, -7.45017529e-02,  5.18085994e-02,\n",
       "       -1.02044821e-01,  6.68749139e-02,  6.14866661e-03, -2.42169630e-02,\n",
       "        4.14351188e-02,  3.73234116e-02, -1.05341054e-01, -3.87899280e-02,\n",
       "       -1.89307574e-02,  1.12819979e-02,  1.59884430e-02, -3.64146568e-02,\n",
       "        1.59431808e-02, -3.82083058e-02,  1.31699275e-02,  3.24053392e-02,\n",
       "        4.93292250e-02,  8.56819190e-03,  6.51695877e-02,  3.09058418e-03,\n",
       "       -4.28010747e-02, -2.97948588e-02,  3.60241416e-03,  2.96271928e-02,\n",
       "        4.49631289e-02, -4.33580298e-03, -3.52360494e-02,  2.98100431e-02,\n",
       "       -3.19973566e-02,  1.23292357e-02, -3.45198251e-02,  5.62834069e-02,\n",
       "        6.59378171e-02,  6.64285794e-02, -1.22487642e-01, -2.07624976e-02,\n",
       "       -1.43723667e-01, -3.59559385e-03,  5.70970820e-03,  5.32657653e-02,\n",
       "        4.98066284e-02,  1.27124516e-02, -8.03760290e-02,  1.30471885e-01,\n",
       "       -1.19293958e-01, -5.47780320e-02, -1.24584459e-01, -2.53754668e-02,\n",
       "        2.01431215e-02, -5.43031935e-03,  8.60709921e-02,  8.05088878e-02,\n",
       "        9.18503013e-03, -1.08206570e-02, -6.31916523e-02,  1.30184013e-02,\n",
       "       -2.68041417e-02, -8.93157795e-02,  2.15253420e-02,  5.99158801e-33,\n",
       "       -5.82671389e-02,  6.32948503e-02, -1.42709743e-02,  3.66161540e-02,\n",
       "        1.96628906e-02,  7.95244356e-04, -9.91934910e-02, -5.12640961e-02,\n",
       "       -1.52651131e-01,  6.39386801e-03, -1.11118406e-02, -3.79301682e-02,\n",
       "       -8.10333043e-02, -1.80831440e-02, -5.28301746e-02, -5.18805422e-02,\n",
       "       -8.33021775e-02, -6.73120022e-02,  1.83864478e-02, -7.26274326e-02,\n",
       "        4.40056883e-02,  5.37311397e-02,  1.31407185e-02,  3.63412239e-02,\n",
       "       -3.81896757e-02, -6.04211399e-03, -2.14581843e-02, -2.11499296e-02,\n",
       "       -5.78486472e-02,  1.83935147e-02, -1.82564408e-02, -5.14777331e-03,\n",
       "       -2.68939510e-02, -3.39582413e-02, -8.33962392e-03, -1.42732989e-02,\n",
       "        9.85073764e-03, -5.89182116e-02, -5.01781888e-02, -4.65756878e-02,\n",
       "       -1.83098558e-02,  4.57885116e-02, -3.25103477e-02, -5.51927686e-02,\n",
       "       -6.50306121e-02,  4.13922593e-02, -9.05254949e-03,  1.34010622e-02,\n",
       "        6.35887980e-02,  4.07582037e-02, -6.70894086e-02, -1.88892484e-02,\n",
       "       -1.16188787e-02, -3.11982655e-03,  2.66688131e-02, -1.33007038e-02,\n",
       "       -1.57612860e-02, -2.70632617e-02,  3.53816375e-02, -2.20901445e-02,\n",
       "        1.05830627e-02,  1.12485525e-03,  1.52628757e-02, -8.21734741e-02,\n",
       "       -4.69620973e-02,  9.17598084e-02, -3.28588262e-02, -6.12141304e-02,\n",
       "        5.48412018e-02,  1.27908494e-02, -1.59391053e-02, -3.89199541e-03,\n",
       "        8.42590854e-02,  3.89574915e-02, -6.99493587e-02,  1.57656632e-02,\n",
       "       -6.73651621e-02, -2.57140510e-02, -2.73305103e-02, -1.19342385e-02,\n",
       "        2.76995171e-02, -3.35533358e-02,  1.48451366e-02, -1.23254058e-03,\n",
       "        8.17653462e-02, -3.03056985e-02,  5.25913946e-03, -9.24829021e-02,\n",
       "        2.51963306e-02,  7.41943568e-02, -6.26498600e-04,  3.58807333e-02,\n",
       "       -3.68188769e-02, -4.80004996e-02,  3.27143483e-02, -6.47853802e-33,\n",
       "       -1.68865882e-02,  6.92025246e-03,  1.56405736e-02, -3.63416485e-02,\n",
       "       -4.31013964e-02, -4.69754227e-02,  2.27041617e-02,  3.43874656e-02,\n",
       "        1.23788878e-01,  6.76922575e-02,  5.14581762e-02,  3.52887250e-02,\n",
       "       -2.09991541e-02, -5.18862158e-02,  5.34421159e-03,  2.26117987e-02,\n",
       "        4.84809279e-02, -2.38430109e-02, -2.45473552e-02, -4.66914997e-02,\n",
       "       -1.43470475e-02,  1.05842046e-01, -5.65352440e-02,  1.13582574e-01,\n",
       "       -1.59904137e-02,  1.03391357e-01,  4.57938686e-02,  6.46128878e-02,\n",
       "       -2.29431186e-02, -5.73313907e-02,  5.34790717e-02, -1.21975936e-01,\n",
       "       -6.17454313e-02,  1.09811805e-01, -7.68405665e-03, -3.33023630e-02,\n",
       "        5.84780201e-02,  3.28458287e-02, -1.37267448e-02,  8.02092180e-02,\n",
       "        1.92318819e-02,  5.30178770e-02, -2.96001937e-02, -2.51600314e-02,\n",
       "       -5.27379662e-02, -4.29850928e-02,  2.26130765e-02, -3.99690121e-02,\n",
       "        3.70388627e-02, -4.96767014e-02,  8.82955715e-02, -3.22078541e-02,\n",
       "       -2.76811011e-02,  2.09320281e-02, -8.79623517e-02,  2.57969392e-03,\n",
       "        9.51633900e-02, -2.95897368e-02,  3.57355177e-02,  2.04732809e-02,\n",
       "        8.05459470e-02,  9.83200893e-02, -4.96363789e-02,  9.17213224e-03,\n",
       "        1.92126110e-02, -1.16012104e-01, -1.49184503e-02,  9.00919084e-04,\n",
       "        6.52694032e-02, -2.54325811e-02, -5.19337133e-02,  2.54506320e-02,\n",
       "       -1.34845860e-02, -5.50983474e-02,  1.21875592e-02,  4.60969582e-02,\n",
       "        1.42533155e-02,  4.96655703e-02,  2.96652522e-02, -2.92824283e-02,\n",
       "        6.43783584e-02,  2.00238880e-02,  3.01431995e-02,  5.86301833e-02,\n",
       "        3.55899036e-02, -2.19117310e-02,  1.77478604e-02,  6.42919242e-02,\n",
       "        7.24564791e-02, -2.20337249e-02,  4.87047583e-02, -2.79316329e-03,\n",
       "       -4.82812524e-04,  6.49163127e-02, -3.42270434e-02, -5.52384094e-08,\n",
       "       -2.72801332e-02, -9.48703885e-02,  3.39974998e-03,  4.74378690e-02,\n",
       "        2.30809916e-02, -9.27758869e-03,  3.76497060e-02, -4.76932898e-02,\n",
       "       -1.99695639e-02,  2.76203007e-02,  7.37646595e-02, -7.59200985e-03,\n",
       "       -1.41164452e-01, -2.38639619e-02, -5.75837195e-02, -4.70689610e-02,\n",
       "       -4.67728674e-02,  1.29361480e-01, -2.86895153e-03,  1.03708757e-02,\n",
       "        1.31164650e-02,  4.01415527e-02, -4.08513993e-02, -8.69908649e-03,\n",
       "        5.53845428e-03,  2.28741905e-03, -5.35771996e-02,  8.56888015e-04,\n",
       "        7.58325979e-02, -7.73119647e-03, -1.70554314e-02,  5.03908135e-02,\n",
       "        6.16321899e-02, -3.71275283e-02, -2.44328473e-02,  3.88082489e-03,\n",
       "        4.55423445e-02,  5.58175929e-02, -1.88149456e-02, -1.65749118e-02,\n",
       "        6.56105131e-02, -2.33943779e-02, -6.95297197e-02,  3.95079106e-02,\n",
       "        6.22829981e-03,  4.98895459e-02, -5.65438531e-02, -4.24544066e-02,\n",
       "        3.99704650e-02, -1.25265988e-02,  1.03317104e-01, -7.03760386e-02,\n",
       "        5.62869646e-02,  3.99252623e-02, -8.31823424e-02, -7.16392621e-02,\n",
       "       -4.17164005e-02,  6.61610216e-02,  2.75908168e-02, -4.78810556e-02,\n",
       "        1.20655008e-01,  5.11448656e-04,  6.07389435e-02,  9.23785269e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ba5cc48-a274-4a9d-8003-961cecd6839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old database files if any\n",
    "persist_directory = os.path.join(os.getcwd(), 'docs', 'chroma')\n",
    "\n",
    "os.makedirs(persist_directory, exist_ok=True)\n",
    "\n",
    "shutil.rmtree(persist_directory, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb476b35-a929-4e07-8c2e-b85c5bf9a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 50\n",
    ")\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create the vector database\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2049e6-617e-4792-8128-ba1e4abfe9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a852268-b1a1-47d4-8e33-32d631a419f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, top_k: int = 5):\n",
    "    # Perform the search\n",
    "    docs_mmr = vectordb.max_marginal_relevance_search(query,k=top_k)\n",
    "    \n",
    "    # Format the results\n",
    "    formatted_results = []\n",
    "    for doc in docs_mmr:\n",
    "        formatted_results.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata,\n",
    "        })\n",
    "    \n",
    "    return formatted_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e885899-f212-46f4-9690-86ad01d0f851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': 'NEW CARSUSED CARSREVIEWS & NEWSADTata Nexon User ReviewsLooking for Tata Nexon? Here are the reviews and ratings by Nexon owners from across the country.4.6/5391 Ratings5 star75%4 star17%3 star3%2 star1%1 star3%VariantAll VersionsRs. 7,99,990Avg. Ex-ShowroomSelect Your VariantAll VersionsAll VersionsPure 1.2 Petrol 6MTPetrolManualRatings 4.6Smart Plus 1.2 Petrol 5MTPetrolManualRatings 4.6Creative 1.2 Petrol 6MTPetrolManualRatings 4.1Creative 1.2 Petrol 6AMTPetrolAutomaticRatings 4.4Smart Plus',\n",
       " 'metadata': {'description': 'Tata Nexon Reviews - Read first-hand reviews from actual Tata Nexon owners. Find out what buyers of Tata Nexon have to say about the car.',\n",
       "  'language': 'en',\n",
       "  'source': 'https://www.carwale.com/tata-cars/nexon/user-reviews-p2/',\n",
       "  'title': 'Tata Nexon Reviews - CarWale'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_search(\"What do users say about the Tata Nexon's fuel economy?\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f811b65b-3e30-4ecc-878f-7028402cdb23",
   "metadata": {},
   "source": [
    "# Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cceb42a4-f4ff-4496-ac19-418536defc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "/home/sukuna/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/quantizers/auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Majipa/cars_base\",\n",
    "                                             device_map=\"cuda\",\n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "103a432c-6b52-4049-b417-093d95fe7271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") \n",
    "\n",
    "pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    ") \n",
    "\n",
    "generation_args = { \n",
    "    \"max_new_tokens\": 500, \n",
    "    \"temperature\": 0., \n",
    "    \"return_full_text\": False,\n",
    "} \n",
    "\n",
    "# output = pipe(messages, **generation_args) \n",
    "# print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "484e5861-efa5-4a3d-aa33-385cae0b3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(context, question):\n",
    "    messages = [ \n",
    "{\"role\": \"system\", \"content\": \"You are a helpful Car Improvement anaylst assistant that works on the basis of provied Reviws.\"},     \n",
    "        {\"role\": \"user\", \"content\":f\"context: {context} question: {question}\"}, \n",
    "        ] \n",
    "    output = pipe(messages, **generation_args) \n",
    "    print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547b7067-5eb2-4b0e-964f-1a610d8c8e50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sukuna/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 37.06 MiB is free. Including non-PyTorch memory, this process has 3.77 GiB memory in use. Of the allocated memory 3.37 GiB is allocated by PyTorch, and 332.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat is the car that we are talking about?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43manswer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m, in \u001b[0;36manswer\u001b[0;34m(context, question)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manswer\u001b[39m(context, question):\n\u001b[1;32m      2\u001b[0m     messages \u001b[38;5;241m=\u001b[39m [ \n\u001b[1;32m      3\u001b[0m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful Car Improvement anaylst assistant that works on the basis of provied Reviws.\u001b[39m\u001b[38;5;124m\"\u001b[39m},     \n\u001b[1;32m      4\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}, \n\u001b[1;32m      5\u001b[0m         ] \n\u001b[0;32m----> 6\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_args\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:257\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    253\u001b[0m     text_inputs, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, KeyDataset) \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[1;32m    254\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text_inputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 257\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mChat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m         chats \u001b[38;5;241m=\u001b[39m [Chat(chat) \u001b[38;5;28;01mfor\u001b[39;00m chat \u001b[38;5;129;01min\u001b[39;00m text_inputs]  \u001b[38;5;66;03m# 🐈 🐈 🐈\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/pipelines/base.py:1254\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1247\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1248\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1251\u001b[0m         )\n\u001b[1;32m   1252\u001b[0m     )\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/pipelines/base.py:1261\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1260\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1261\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1262\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/pipelines/base.py:1161\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1160\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1161\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:351\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/generation/utils.py:1989\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1981\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1982\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1983\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1984\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1985\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1986\u001b[0m     )\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 1989\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1990\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1995\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1997\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2001\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2003\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2004\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[1;32m   2005\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/generation/utils.py:2932\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[1;32m   2929\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   2931\u001b[0m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2932\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2935\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:1029\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1026\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1029\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1043\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:804\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    793\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    794\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    795\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         cache_position,\n\u001b[1;32m    802\u001b[0m     )\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 804\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:560\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    559\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 560\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    563\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/accelerate/hooks.py:169\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py:153\u001b[0m, in \u001b[0;36mMistralMLP.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(hidden_state)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/LLM/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:489\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dora[active_adapter]:\n\u001b[0;32m--> 489\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mlora_B\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     x \u001b[38;5;241m=\u001b[39m dropout(x)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 110.00 MiB. GPU 0 has a total capacity of 3.81 GiB of which 37.06 MiB is free. Including non-PyTorch memory, this process has 3.77 GiB memory in use. Of the allocated memory 3.37 GiB is allocated by PyTorch, and 332.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "question = \"what is the car that we are talking about?\"\n",
    "answer(docs,question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf60d57e-d79e-490a-b2d1-80ee00d445fe",
   "metadata": {},
   "source": [
    "# Data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11eb721-5532-4b46-8460-75397558972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b3ddc76-f96e-44d1-b75e-21fd565b8a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv(\"tata_punch_reviews.csv\", encoding='latin-1', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01dbaf20-848a-4925-b0a5-931f5bf7da8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewer</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>narendra singh</td>\n",
       "      <td>5</td>\n",
       "      <td>Fantastic Car In Budget</td>\n",
       "      <td>I recently purchased the Tata Punch, and I cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>5</td>\n",
       "      <td>Best Quality Car</td>\n",
       "      <td>Tata Punch is highly regarded as a suitable ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>debee prasad sahoo</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Best Safety Car For Middleclass People</td>\n",
       "      <td>It's very comfortable and it holds 4 people's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kunwarsa production</td>\n",
       "      <td>5</td>\n",
       "      <td>Best Car</td>\n",
       "      <td>The Tata Punch stands out as the top mini SUV ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aman ali</td>\n",
       "      <td>5</td>\n",
       "      <td>Best Car</td>\n",
       "      <td>Had a wonderful experience with Tata Punch,?he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>armaan negi</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Best SUV Of Tata.</td>\n",
       "      <td>Nice car to drive. Nice performance. But it ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>851</th>\n",
       "      <td>bishnu halder</td>\n",
       "      <td>5</td>\n",
       "      <td>Nice Car</td>\n",
       "      <td>It is a nice car with a better road presence?n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>852</th>\n",
       "      <td>soumyadip dutta</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Great Experience Overall</td>\n",
       "      <td>Best micro SUV in the range of 6-8 lakhs with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>ganesh pudduchery</td>\n",
       "      <td>5</td>\n",
       "      <td>5 Star Safety Rated Heavy, Rigid, Spacious SUV...</td>\n",
       "      <td>I Bought a Tata punch adventure a week ago, Ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>s kumar</td>\n",
       "      <td>5</td>\n",
       "      <td>Strong And Comfortable Machine</td>\n",
       "      <td>Great car within budget. I am getting a mileag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>855 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Reviewer Rating  \\\n",
       "0         narendra singh      5   \n",
       "1                  sunny      5   \n",
       "2     debee prasad sahoo    4.5   \n",
       "3    kunwarsa production      5   \n",
       "4               aman ali      5   \n",
       "..                   ...    ...   \n",
       "850          armaan negi    4.7   \n",
       "851       bishnu halder       5   \n",
       "852      soumyadip dutta    4.7   \n",
       "853    ganesh pudduchery      5   \n",
       "854              s kumar      5   \n",
       "\n",
       "                                                 Title  \\\n",
       "0                              Fantastic Car In Budget   \n",
       "1                                     Best Quality Car   \n",
       "2               Best Safety Car For Middleclass People   \n",
       "3                                             Best Car   \n",
       "4                                             Best Car   \n",
       "..                                                 ...   \n",
       "850                                  Best SUV Of Tata.   \n",
       "851                                           Nice Car   \n",
       "852                           Great Experience Overall   \n",
       "853  5 Star Safety Rated Heavy, Rigid, Spacious SUV...   \n",
       "854                     Strong And Comfortable Machine   \n",
       "\n",
       "                                               Content  \n",
       "0    I recently purchased the Tata Punch, and I cou...  \n",
       "1    Tata Punch is highly regarded as a suitable ca...  \n",
       "2    It's very comfortable and it holds 4 people's ...  \n",
       "3    The Tata Punch stands out as the top mini SUV ...  \n",
       "4    Had a wonderful experience with Tata Punch,?he...  \n",
       "..                                                 ...  \n",
       "850  Nice car to drive. Nice performance. But it ha...  \n",
       "851  It is a nice car with a better road presence?n...  \n",
       "852  Best micro SUV in the range of 6-8 lakhs with ...  \n",
       "853  I Bought a Tata punch adventure a week ago, Ea...  \n",
       "854  Great car within budget. I am getting a mileag...  \n",
       "\n",
       "[855 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e82a43fe-7f40-4c8d-bd88-da34e2eb232b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviewer</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>narendra singh</td>\n",
       "      <td>5</td>\n",
       "      <td>Fantastic Car In Budget</td>\n",
       "      <td>I recently purchased the Tata Punch, and I cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>5</td>\n",
       "      <td>Best Quality Car</td>\n",
       "      <td>Tata Punch is highly regarded as a suitable ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>debee prasad sahoo</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Best Safety Car For Middleclass People</td>\n",
       "      <td>It's very comfortable and it holds 4 people's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kunwarsa production</td>\n",
       "      <td>5</td>\n",
       "      <td>Best Car</td>\n",
       "      <td>The Tata Punch stands out as the top mini SUV ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aman ali</td>\n",
       "      <td>5</td>\n",
       "      <td>Best Car</td>\n",
       "      <td>Had a wonderful experience with Tata Punch,?he...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Reviewer Rating                                   Title  \\\n",
       "0       narendra singh      5                 Fantastic Car In Budget   \n",
       "1                sunny      5                        Best Quality Car   \n",
       "2   debee prasad sahoo    4.5  Best Safety Car For Middleclass People   \n",
       "3  kunwarsa production      5                                Best Car   \n",
       "4             aman ali      5                                Best Car   \n",
       "\n",
       "                                             Content  \n",
       "0  I recently purchased the Tata Punch, and I cou...  \n",
       "1  Tata Punch is highly regarded as a suitable ca...  \n",
       "2  It's very comfortable and it holds 4 people's ...  \n",
       "3  The Tata Punch stands out as the top mini SUV ...  \n",
       "4  Had a wonderful experience with Tata Punch,?he...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cc4e815-793c-4b36-bef5-42eab313261c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been written to output.txt\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is named df\n",
    "with open('output.txt', 'w', encoding='utf-8') as file:\n",
    "    for index, row in x.iterrows():\n",
    "        # Format each row as desired\n",
    "        formatted_line = f\"Reviewer: {row['Reviewer']}\\n\"\n",
    "        formatted_line += f\"Rating: {row['Rating']}\\n\"\n",
    "        formatted_line += f\"Title: {row['Title']}\\n\"\n",
    "        formatted_line += f\"Content: {row['Content']}\\n\"\n",
    "        formatted_line += \"-\" * 50 + \"\\n\"  # Separator between entries\n",
    "        \n",
    "        # Write the formatted line to the file\n",
    "        file.write(formatted_line)\n",
    "\n",
    "print(\"Data has been written to output.txt\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "632ec924-72a2-4814-a3ed-bfe340d8b77c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
